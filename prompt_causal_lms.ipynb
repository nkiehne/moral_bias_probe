{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9544ba57-5cc9-4e17-992e-dcfe068c9a93",
   "metadata": {},
   "source": [
    "# Prompting masked lms\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713262ab-eac2-4cd2-99e2-74adc3a55f9e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_gpus = 1\n",
    "model_name = \"gpt2\"\n",
    "logdir = \"data/models/tests/\"\n",
    "prompt_dir = \"data/prompts/topics/\"\n",
    "from_checkpoint = None #\"data/models/masked_classification/contrastive-moral-stories/gpt2/bs32_lr_1e-05/\"\n",
    "# whether from_checkpoints points to a directory of multiple checkpoints for the same architecture\n",
    "# if True, this script will load the weights consecutively without creating the model again for each of the state_dicts\n",
    "# This saves a lot of time.\n",
    "# Note: `from_checkpoint` is expected to point to a dir of dirs, each of which are valid arguments as singular runs\n",
    "#multi_checkpoints = False\n",
    "override_logdir = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d288c55-7143-4576-8f11-6ec049b9e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"8\"\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "import datasets\n",
    "from social_chem import load_ms_soc_joined\n",
    "import fastmodellib as fml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296e1a8-1ed6-40af-987f-9256b14e7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=logdir,\n",
    "    overwrite_output_dir=override_logdir,\n",
    "    logging_dir=logdir,\n",
    "    report_to=\"tensorboard\",\n",
    "    #include_inputs_for_metrics=True,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #eval_accumulation_steps=32,\n",
    "    fp16=True,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab0b1f-c82d-49f3-9861-df3590bc2da1",
   "metadata": {},
   "source": [
    "## Preparing args\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573e8e8-1361-4038-9f84-6b95632e83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find checkpoint\n",
    "import pathlib\n",
    "if from_checkpoint is not None:\n",
    "    print(\"Checkpoint given:\", from_checkpoint)\n",
    "    if fml.persistence.is_checkpoint_dir(from_checkpoint):\n",
    "        checkpoints = [from_checkpoint]\n",
    "        print(\"Checkpoint was found\", checkpoints)\n",
    "    else:\n",
    "        p = pathlib.Path(from_checkpoint)\n",
    "        checkpoints = [str(x) for x in p.glob(\"checkpoint-*\") if fml.persistence.is_checkpoint_dir(x)]\n",
    "        print(\"Found checkpoints in subdirectories:\", checkpoints)\n",
    "    if len(checkpoints) == 0:\n",
    "        raise ValueError(f\"Found no checkpoint in dir '{from_checkpoint}'\")\n",
    "else:\n",
    "    checkpoints = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820f26d-194f-41e7-a923-92507096f7f7",
   "metadata": {},
   "source": [
    "# ensure checkpoints are a list or None\n",
    "if multi_checkpoints:\n",
    "    if from_checkpoint is None:\n",
    "        raise ValueError(\"Need a valid directory for parameter `from_checkpoint`\")\n",
    "    if isinstance(from_checkpoint, str):\n",
    "        # extract paths\n",
    "        checkpoints = fml.persistence.find_checkpoints(from_checkpoint)\n",
    "    elif isinstance(from_checkpoint, list):\n",
    "        checkpoints = from_checkpoint\n",
    "else:\n",
    "    # assume single checkpoint\n",
    "    checkpoints = [from_checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1a2b9-9ff0-41d7-be65-95bd9ece46b0",
   "metadata": {},
   "source": [
    "## Loading model + tokenizer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120bce9-5588-4f61-b321-b7834b86a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# set tokenizer padding to right!\n",
    "# looking at you, llama\n",
    "tokenizer.padding_side = \"right\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b354ce-adce-436c-b7e1-665214b5549c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct the model with the first checkpoint\n",
    "model = fml.load_model(model_name=model_name, from_checkpoint=checkpoints[0], load_pretrained_weights=True, model_class=AutoModelForCausalLM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f21eb8-cbf4-41c6-83a9-4340b82da024",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfaeaa-a693-400a-94bf-c013fe922d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opinion_lexicon():\n",
    "    with open(\"data/opinion-lexicon-English/negative-words.txt\", encoding=\"latin1\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [x.strip() for x in lines if not x.startswith(\";\")]\n",
    "    negative = [x for x in lines if len(x) > 0]\n",
    "    with open(\"data/opinion-lexicon-English/positive-words.txt\", encoding=\"latin1\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [x.strip() for x in lines if not x.startswith(\";\")]\n",
    "    positive = [x for x in lines if len(x) > 0]\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c4cd7-7ad8-4fd5-8e96-94547f25a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive, negative = load_opinion_lexicon()\n",
    "\n",
    "# We expect gpt like models to generate whole words with a whitespace in front\n",
    "# Empirically, gpt like models have very few non-whitespace words\n",
    "# llama, however, behaves much more like bert-like models, i.e. it no words\n",
    "# with whitespaces in front\n",
    "if \"llama\" not in model_name:\n",
    "    positive = [\" \" + x for x in positive]\n",
    "    negative = [\" \" + x for x in negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff091d0-7875-4da6-a20e-29f9a302126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = {p:t for p,t in zip(positive, tokenizer(positive, add_special_tokens=False)[\"input_ids\"]) if len(t) == 1}\n",
    "neg_enc = {p:t for p,t in zip(negative, tokenizer(negative, add_special_tokens=False)[\"input_ids\"]) if len(t) == 1}\n",
    "\n",
    "pos_ids = sum(pos_enc.values(), [])\n",
    "neg_ids = sum(neg_enc.values(), [])\n",
    "\n",
    "all_ids = pos_ids + neg_ids\n",
    "\n",
    "print(\"Positive words:\", len(pos_ids))\n",
    "print(\"Negative words:\", len(neg_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f1cf7-eb28-4aeb-ac29-d5582a0e757a",
   "metadata": {},
   "source": [
    "### Loading prompts\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4946ff5-16db-4813-bda2-e07d597b1cda",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_files = [x for x in os.listdir(prompt_dir) if x.endswith(\".jsonl\")]\n",
    "dataset = datasets.DatasetDict()\n",
    "pos_label_word = next(iter(pos_enc.keys()))\n",
    "neg_label_word = next(iter(neg_enc.keys()))\n",
    "\n",
    "for pf in prompt_files:\n",
    "    d = pd.read_json(prompt_dir + pf, orient=\"records\", lines=True)\n",
    "\n",
    "    # 1: norm has positive moral judgment, 0 negative\n",
    "    d[\"original_label\"] = (d[\"action-moral-judgment\"] > 0).astype(\"int32\")\n",
    "\n",
    "    # find all prompts, that end with a mask\n",
    "    mask_end = d[\"prompt\"].map(lambda x: x.endswith(\"[MASK].\\\"\") or x.endswith(\"[MASK].\"))\n",
    "    \n",
    "    d = d[mask_end]\n",
    "    \n",
    "    if len(d) == 0: continue\n",
    "\n",
    "    print(pf)\n",
    "    d[\"prompt\"] = d[\"prompt\"].apply(lambda x: x.removesuffix(\"[MASK].\\\"\").removesuffix(\"[MASK].\"))\n",
    "    # we create artificial text targets with a random positive or negative word.\n",
    "    # this way, we can infer whether an input should have been a positive or a negative norm during metric computation\n",
    "    d[\"prompt\"] += d[\"original_label\"].map(lambda x: pos_label_word.strip() if x == 1 else neg_label_word.strip())\n",
    "\n",
    "    dataset[os.path.splitext(pf)[0]] = datasets.Dataset.from_pandas(d)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} prompt tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e20fba-e1b4-40df-a24d-fcd9162cd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    return tokenizer(samples[\"prompt\"], padding=False)\n",
    "\n",
    "tokenized_data = dataset.map(tokenize, batched=True, batch_size=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a06da-d953-4077-a91c-4e95516f1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    probs = torch.tensor(eval_pred.predictions)\n",
    "    labels = torch.tensor(eval_pred.label_ids)\n",
    "\n",
    "    '''\n",
    "    Short explanation:\n",
    "    We input our left-to-right models with full prompts, that is, the desired answer is included in the input ids.\n",
    "    But: These models do not have access to future tokens to predict the current one. Therefore, this is safe in terms of training.\n",
    "    For the evaluation we need two infos: Which token would have been correct and what did the model predict \n",
    "    The first can be gathered from the label ids by finding the word before the collator padding (-100).\n",
    "    To find the model prediction we need to move one index to left!     \n",
    "    '''\n",
    "    # we need to find the indices of the last predicted word\n",
    "    # these differ across samples due to right padding...\n",
    "    # here, we assume that the collator padded sentences with -100\n",
    "    if tokenizer.padding_side == \"right\":\n",
    "        label_pos = (labels == -100).float().argmax(-1)\n",
    "        label_pos[label_pos == 0] = labels.shape[-1]\n",
    "        label_pos = label_pos - 1\n",
    "        r = range(label_pos.shape[0])\n",
    "        y_pred = probs[r,label_pos-1]\n",
    "        correct_ids = labels[r, label_pos]\n",
    "    else:\n",
    "        label_pos = torch.zeros(labels.shape[0], dtype=torch.long)\n",
    "\n",
    "    # if it is not a positive word id, then it is negative\n",
    "    # here we assume, that the input was genereated correctly\n",
    "    y_true = torch.isin(correct_ids, torch.tensor(pos_ids))\n",
    "\n",
    "    acc = (y_true == y_pred).type(torch.float32).mean()\n",
    "    return {\"accuracy\":acc, \"y_pred\":y_pred.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3996a5-34df-4982-9fd0-fe3a12430338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reduce the number of returned logits by 30kx fold to safe vram!\n",
    "pos_tensor = torch.tensor(pos_ids, device=model.device)\n",
    "neg_tensor = torch.tensor(neg_ids, device=model.device)\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        logits = logits[0]\n",
    "    probs = torch.softmax(logits, -1)\n",
    "    # lets pre-compute the sums of positive and negative probabilities\n",
    "    # this way, we only need to store [batch_size x seq_len x 2] bool values\n",
    "    # before this, we needed [batch_size x seq_len x vocab_size]\n",
    "    pos_probs = probs[:,:,pos_tensor].sum(axis=-1)\n",
    "    neg_probs = probs[:,:,neg_tensor].sum(axis=-1)\n",
    "    y_pred = pos_probs > neg_probs\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f1a5f-399d-4025-ac62-8a8b14a956c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# set padding token if necessary\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "dc = DataCollatorForLanguageModeling(tokenizer, mlm=False, pad_to_multiple_of=8, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790105e-0ccc-491e-876b-7d1ac3eed474",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=dc,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b289e6-4f5e-45b0-b696-c75b73c14ea9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for split, data in tokenized_data.items():\n",
    "    r = trainer.evaluate(data, metric_key_prefix=f\"{split}\")\n",
    "    results[split] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421a90de-37eb-4284-8f36-15cd678c2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "preds = OrderedDict({k:pd.DataFrame(v[f\"{k}_y_pred\"]) for k, v in results.items()})\n",
    "\n",
    "all_preds = reduce(lambda l,r: pd.concat([l,r], axis=1), preds.values())\n",
    "all_preds.columns = preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ef516-dea7-4ddd-af30-0d1ee029a818",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(logdir + \"prompt_results.jsonl\", \"w\") as f:\n",
    "    f.write(all_preds.to_json(orient=\"records\", lines=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
