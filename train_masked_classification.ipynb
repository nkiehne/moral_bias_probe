{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "047f67c2-ba69-415e-be50-9655d4cf2784",
   "metadata": {},
   "source": [
    "# Prepare Experiment & Deepspeed config (**MANDATORY**)\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9ab32-f86e-4830-ac5f-0b9dcd9fceb6",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "ds_config = {\n",
    "    \"fp16\": {\n",
    "        \"enabled\": \"auto\",\n",
    "        \"loss_scale\": 0,\n",
    "        \"loss_scale_window\": 1000,\n",
    "        \"initial_scale_power\": 16,\n",
    "        \"hysteresis\": 2,\n",
    "        \"min_loss_scale\": 1\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\n",
    "            \"lr\": \"auto\",\n",
    "            \"betas\": \"auto\",\n",
    "            \"eps\": \"auto\",\n",
    "            \"weight_decay\": \"auto\"\n",
    "        }\n",
    "    },\n",
    "\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": True,\n",
    "        \"allgather_bucket_size\": 5e8,\n",
    "        \"overlap_comm\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "        \"reduce_bucket_size\": 5e8,\n",
    "        \"contiguous_gradients\": True,\n",
    "    },\n",
    "    \"gradient_accumulation_steps\": \"auto\",\n",
    "    \"gradient_clipping\": \"auto\",\n",
    "    \"steps_per_print\": 200,\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "    \"wall_clock_breakdown\": False\n",
    "}\n",
    "\n",
    "training_args = {\n",
    "    \"do_train\": True,\n",
    "    \"do_eval\": True,\n",
    "    \"do_predict\": False,\n",
    "    \"num_train_epochs\": 4,\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"per_device_train_batch_size\": 64,\n",
    "    \"per_device_eval_batch_size\": 32,\n",
    "    \"fp16\": True,\n",
    "    \"weight_decay\": 0.0,\n",
    "    \"warmup_steps\": 0,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"logging_strategy\": \"epoch\",\n",
    "    \"evaluation_strategy\": \"epoch\",\n",
    "    \"save_strategy\": \"epoch\",\n",
    "    \"save_total_limit\": 1,\n",
    "    \"load_best_model_at_end\": True,\n",
    "    \"metric_for_best_model\": \"eval_accuracy\",\n",
    "    \"greater_is_better\": True,\n",
    "}\n",
    "\n",
    "model_args = {\n",
    "}\n",
    "\n",
    "# usually overriden by external config:\n",
    "num_gpus = 1\n",
    "model_name =\"bert-base-uncased\"\n",
    "logdir = \"data/models/bert-base-uncased/ms/\"\n",
    "override_logdir = True\n",
    "\n",
    "dataset = \"swag\"\n",
    "seed = 8197\n",
    "\n",
    "#dataset_folder = \"data/moral_stories_datasets/classification/action+norm/norm_distance/\"\n",
    "load_pretrained_weights = True\n",
    "from_checkpoint = None\n",
    "deepspeed = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28b194b8-be8d-44e3-90dc-27f3fe4d05ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if deepspeed == False:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoConfig, AutoModelForMaskedLM\n",
    "import datasets\n",
    "from transformers import Trainer, TrainingArguments\n",
    "import fastmodellib as fml\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c3be102-901c-4946-8e8e-3411f5a47daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=logdir,\n",
    "    overwrite_output_dir=override_logdir,\n",
    "    logging_dir=logdir,\n",
    "    deepspeed= logdir + \"/ds_config.json\" if deepspeed else None,\n",
    "    report_to=\"tensorboard\",\n",
    "    include_inputs_for_metrics=True,\n",
    "    #eval_accumulation_steps=8,\n",
    "    **training_args\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6193ccdf-bc57-42e5-9b21-ade4db40bee5",
   "metadata": {},
   "source": [
    "# Tokenize the dataset\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f17977e-bd00-487e-af12-61a3917746ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these words are considered as answers to the prompt and will be searched for in the output of the model\n",
    "positive_label_word = \"yes\"\n",
    "negative_label_word = \"no\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c709d7-c6a8-4ac4-9280-9b3066705abd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"Eleuther\" in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, bos_token='<|startoftext|>', \n",
    "                                          eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5b7c7d-7dd6-4b43-b117-ae993fd0ca8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_action_norm_split(path):\n",
    "    # loads the moral stories like data under path\n",
    "    train, dev, test = [pd.read_json(f\"{path}{x}.jsonl\", lines=True) for x in [\"train\", \"dev\", \"test\"]]\n",
    "\n",
    "    # construct dataframes that can actually be used\n",
    "    assign_action = lambda x: x[\"moral_action\"] if x[\"label\"] == 1 else x[\"immoral_action\"]\n",
    "    train[\"action\"] = train.apply(assign_action, axis=1)\n",
    "    dev[\"action\"] = dev.apply(assign_action, axis=1)\n",
    "    test[\"action\"] = test.apply(assign_action, axis=1)\n",
    "\n",
    "    subset = [\"norm\", \"action\", \"label\"]\n",
    "    train = train[subset]\n",
    "    dev = dev[subset]\n",
    "    test = test[subset]\n",
    "    return train, dev, test\n",
    "\n",
    "if dataset==\"moral-stories\" or dataset == \"contrastive-moral-stories\":\n",
    "    if dataset == \"moral-stories\":\n",
    "        dataset_folder = \"data/contrastive_moral_stories/original_ms/action+norm/norm_distance/\"\n",
    "    else:\n",
    "        dataset_folder = \"data/contrastive_moral_stories/anti_ms_llama/action+norm/norm_distance/\"\n",
    "\n",
    "    train, dev, test = load_action_norm_split(dataset_folder)\n",
    "\n",
    "    # chat like prompt\n",
    "    template = \"Consider the norm \\\"{norm}\\\" and the action \\\"{action}\\\". Is the described action moral with respect to the norm? Answer with \\\"yes\\\" if it is moral or \\\"no\\\" if it is immoral.\\nAnswer: {mask}\"\n",
    "\n",
    "    def promptify(data):\n",
    "        data[\"prompt\"] = [template.format(norm=n, action=a, mask=tokenizer.mask_token) for n,a in zip(data[\"norm\"], data[\"action\"])]\n",
    "        data[\"original_label\"] = data[\"label\"]\n",
    "        data[\"label\"] = [template.format(norm=n, action=a, mask=positive_label_word if l==1 else negative_label_word) \\\n",
    "                         for n,a,l in zip(data[\"norm\"], data[\"action\"], data[\"label\"])]\n",
    "        return data\n",
    "\n",
    "    train, dev, test = [promptify(x) for x in [train, dev, test]]\n",
    "\n",
    "    data = datasets.DatasetDict()\n",
    "    data[\"train\"] = datasets.Dataset.from_pandas(train)\n",
    "    data[\"dev\"] = datasets.Dataset.from_pandas(dev)\n",
    "    data[\"test\"] = datasets.Dataset.from_pandas(test)\n",
    "\n",
    "elif dataset==\"tweet-eval\":\n",
    "    te = load_dataset(\"tweet_eval\", \"hate\")\n",
    "    te[\"dev\"] = te.pop(\"validation\")\n",
    "    \n",
    "    template = \"Here is a tweet: \\\"{tweet}\\\". If the tweet contains hate-speech, answer with \\\"yes\\\", or \\\"no\\\" if it doesn't.\\nAnswer: {mask}\"\n",
    "    def promptify(data):\n",
    "        data[\"prompt\"] = [template.format(tweet=t, mask=tokenizer.mask_token) for t in data[\"text\"]]\n",
    "        data[\"original_label\"] = data[\"label\"]\n",
    "        data[\"label\"] = [template.format(tweet=t, mask=positive_label_word if l==1 else negative_label_word) \\\n",
    "                         for t,l in zip(data[\"text\"], data[\"label\"])]\n",
    "        return data\n",
    "\n",
    "    data = te.map(promptify, batched=True, batch_size=1000)\n",
    "\n",
    "elif dataset==\"swag\":\n",
    "    from sklearn.model_selection import train_test_split\n",
    "\n",
    "    swag = load_dataset(\"swag\", \"regular\")\n",
    "\n",
    "    template = \"Does the ending fit the sentence?\\n{ctx}\\n{ending}\\n\\nAnswer: {mask}\"\n",
    "\n",
    "    def prepare_data(data):\n",
    "        data[\"original_label\"] = data[\"label\"]\n",
    "        # if the correct answer is in the first two options, we use the sample as a positive one\n",
    "        # if not, then we use the very first option of the sample (always an incorrect option!) as a false sample\n",
    "        data[\"prompt\"] = data.apply(lambda row: template.format(ctx=row.startphrase, ending=row[f\"ending{row.original_label}\"] if row.original_label in{0,1} else row[\"ending0\"], mask=tokenizer.mask_token), axis=1)\n",
    "        data[\"label\"] = data.apply(lambda x: x.prompt.replace(tokenizer.mask_token, positive_label_word if x.original_label in {0,1} else negative_label_word), axis=1)\n",
    "        # finally, we replace the label with True/False instead of the indices of the answer\n",
    "        data[\"original_label\"] = data[\"original_label\"].apply(lambda x: x in {0,1})\n",
    "        return data\n",
    "\n",
    "    train = swag[\"train\"].to_pandas()\n",
    "    test = swag[\"validation\"].to_pandas()\n",
    "    dev, test = train_test_split(test, test_size=0.5, shuffle=False, random_state=seed)\n",
    "\n",
    "    train, dev, test = [prepare_data(x) for x in [train, dev, test]]\n",
    "\n",
    "    data = datasets.DatasetDict()\n",
    "    data[\"train\"] = datasets.Dataset.from_pandas(train)\n",
    "    data[\"dev\"] = datasets.Dataset.from_pandas(dev)\n",
    "    data[\"test\"] = datasets.Dataset.from_pandas(test)\n",
    "\n",
    "    \n",
    "elif dataset==\"boolq\":\n",
    "    boolq = load_dataset(\"boolq\").shuffle(seed=seed)\n",
    "    train = boolq[\"train\"].to_pandas()\n",
    "    dev = boolq[\"validation\"].to_pandas()\n",
    "    # split into dev and test\n",
    "    n = len(dev)//2\n",
    "    test = dev[n:]\n",
    "    dev = dev[:n]\n",
    "\n",
    "elif dataset==\"rte\":\n",
    "    from datasets import load_dataset\n",
    "else:\n",
    "    raise ValueError(f\"Unknown task '{dataset}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3119c029-6f78-4b9e-bf5d-7a2e722232f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    data = tokenizer(samples[\"prompt\"], text_target=samples[\"label\"], padding=False)\n",
    "    data[\"mask_pos\"] = [x.index(tokenizer.mask_token_id) for x in data[\"input_ids\"]]\n",
    "    return data\n",
    "\n",
    "tokenized_data = data.map(tokenize, batched=True, batch_size=128).shuffle()\n",
    "tokenized_data = tokenized_data.remove_columns(\"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8706d5b-1ade-4155-812c-d9b41e63ba98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to find out, which token the tokenizer chose for the label words\n",
    "\n",
    "# find a positive sample\n",
    "p = next(r for r in tokenized_data[\"train\"] if r[\"original_label\"] == 1)\n",
    "pos_ids = [p[\"labels\"][p[\"mask_pos\"]]]\n",
    "\n",
    "\n",
    "# find a negative sample\n",
    "n = next(r for r in tokenized_data[\"train\"] if r[\"original_label\"] == 0)\n",
    "neg_ids = [n[\"labels\"][n[\"mask_pos\"]]]\n",
    "\n",
    "\n",
    "print(f\"Found {len(pos_ids)} positive and {len(neg_ids)} negative label words:\")\n",
    "print(f\"\\tPositive: '{tokenizer.decode(pos_ids)}'\")\n",
    "print(f\"\\tNegative: '{tokenizer.decode(neg_ids)}'\")\n",
    "\n",
    "if len(pos_ids) == 0 or len(neg_ids) == 0:\n",
    "    raise ValueError(\"Label words are empty!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a8cb7c-de02-4e36-bd84-d422d42dce04",
   "metadata": {},
   "source": [
    "# Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f3ae477-3b48-45b7-b1a8-8ca2d2339e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = fml.load_model(model_name=model_name, from_checkpoint=from_checkpoint, load_pretrained_weights=load_pretrained_weights,\n",
    "                       model_class=AutoModelForMaskedLM, **model_args)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc523063-8995-46bc-bfe1-69fd8defb637",
   "metadata": {},
   "source": [
    "# Prepare Trainer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978849a5-1ba8-4833-9130-383f4a2efc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    probs = torch.tensor(eval_pred.predictions)\n",
    "    input_ids = torch.tensor(eval_pred.inputs)\n",
    "    \n",
    "    mask_probs = probs[input_ids == tokenizer.mask_token_id]\n",
    "    \n",
    "    pos_prob = mask_probs[:, :len(pos_ids)].sum(axis=1)\n",
    "    neg_prob = mask_probs[:, len(pos_ids):].sum(axis=1)\n",
    "    y_pred = pos_prob > neg_prob\n",
    "\n",
    "    # find out which label the samples had\n",
    "    labels = torch.tensor(eval_pred.label_ids)\n",
    "    labels = labels[input_ids == tokenizer.mask_token_id]\n",
    "    # if it is not a positive word id, then it is negative\n",
    "    # here we assume, that the input was genereated correctly\n",
    "    y_true = torch.isin(labels, torch.tensor(pos_ids))\n",
    "    acc = (y_true == y_pred).type(torch.float32).mean()\n",
    "    return {\"accuracy\":acc}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0950c552-5933-46c0-a885-8c8601300c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reduce the number of returned logits by 30kx fold to safe vram!\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    probs = torch.softmax(logits, -1)\n",
    "    return probs[:,:,pos_ids + neg_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489e8a49-d1a9-499b-9cb4-3bb4a94b5f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "dc = DataCollatorForTokenClassification(tokenizer, padding=True, pad_to_multiple_of=8, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c051ce-e7af-4001-be2e-37da8d3b4acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=dc,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_data[\"train\"] if training_args.do_train else None,\n",
    "    eval_dataset=tokenized_data[\"dev\"] if training_args.do_eval else None,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9e31999-8117-4d12-818c-d45cbdaa4344",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if training_args.do_train:\n",
    "    trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cc1a662-2047-4237-b5db-a15a356c5e36",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if training_args.do_predict:\n",
    "    print(\"RUNNING TESTS\")\n",
    "    for split, data in tokenized_data.items():\n",
    "        r = trainer.evaluate(data, metric_key_prefix=f\"test_{split}\")\n",
    "        print(r)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "toc-autonumbering": true
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
