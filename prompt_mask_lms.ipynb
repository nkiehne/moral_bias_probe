{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9544ba57-5cc9-4e17-992e-dcfe068c9a93",
   "metadata": {},
   "source": [
    "# Prompting masked lms\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "713262ab-eac2-4cd2-99e2-74adc3a55f9e",
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_gpus = 1\n",
    "model_name = \"roberta-large\"\n",
    "logdir = \"data/models/tests/\"\n",
    "prompt_dir = \"data/prompts/topics/\"\n",
    "from_checkpoint = None #\"data/models/masked_classification/moral-stories/bert-base-uncased/bs32_lr_0_0001/\"\n",
    "# whether from_checkpoints points to a directory of multiple checkpoints for the same architecture\n",
    "# if True, this script will load the weights consecutively without creating the model again for each of the state_dicts\n",
    "# This saves a lot of time.\n",
    "# Note: `from_checkpoint` is expected to point to a dir of dirs, each of which are valid arguments as singular runs\n",
    "#multi_checkpoints = False\n",
    "override_logdir = True\n",
    "intersect_vocabs = False\n",
    "mask_models = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d288c55-7143-4576-8f11-6ec049b9e7d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "if False:\n",
    "    os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, pipeline, TrainingArguments, Trainer\n",
    "import datasets\n",
    "from social_chem import load_ms_soc_joined\n",
    "import fastmodellib as fml\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b296e1a8-1ed6-40af-987f-9256b14e7568",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=logdir,\n",
    "    overwrite_output_dir=override_logdir,\n",
    "    logging_dir=logdir,\n",
    "    report_to=\"tensorboard\",\n",
    "    include_inputs_for_metrics=True,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    #eval_accumulation_steps=32,\n",
    "    fp16=True,\n",
    "    do_train=False,\n",
    "    do_eval=True,\n",
    "    do_predict=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4ab0b1f-c82d-49f3-9861-df3590bc2da1",
   "metadata": {},
   "source": [
    "## Preparing args\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2573e8e8-1361-4038-9f84-6b95632e83b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find checkpoint\n",
    "import pathlib\n",
    "if from_checkpoint is not None:\n",
    "    print(\"Checkpoint given:\", from_checkpoint)\n",
    "    if fml.persistence.is_checkpoint_dir(from_checkpoint):\n",
    "        checkpoints = [from_checkpoint]\n",
    "        print(\"Checkpoint was found\", checkpoints)\n",
    "    else:\n",
    "        p = pathlib.Path(from_checkpoint)\n",
    "        checkpoints = [str(x) for x in p.glob(\"checkpoint-*\") if fml.persistence.is_checkpoint_dir(x)]\n",
    "        print(\"Found checkpoints in subdirectories:\", checkpoints)\n",
    "    if len(checkpoints) == 0:\n",
    "        raise ValueError(f\"Found no checkpoint in dir '{from_checkpoint}'\")\n",
    "else:\n",
    "    checkpoints = [None]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3820f26d-194f-41e7-a923-92507096f7f7",
   "metadata": {},
   "source": [
    "# ensure checkpoints are a list or None\n",
    "if multi_checkpoints:\n",
    "    if from_checkpoint is None:\n",
    "        raise ValueError(\"Need a valid directory for parameter `from_checkpoint`\")\n",
    "    if isinstance(from_checkpoint, str):\n",
    "        # extract paths\n",
    "        checkpoints = fml.persistence.find_checkpoints(from_checkpoint)\n",
    "    elif isinstance(from_checkpoint, list):\n",
    "        checkpoints = from_checkpoint\n",
    "else:\n",
    "    # assume single checkpoint\n",
    "    checkpoints = [from_checkpoint]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbe1a2b9-9ff0-41d7-be65-95bd9ece46b0",
   "metadata": {},
   "source": [
    "## Loading model + tokenizer\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9120bce9-5588-4f61-b321-b7834b86a17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"Eleuther\" in model_name:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name, bos_token='<|startoftext|>', eos_token='<|endoftext|>', pad_token='<|pad|>')\n",
    "else:\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b354ce-adce-436c-b7e1-665214b5549c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# construct the model with the first checkpoint\n",
    "model = fml.load_model(model_name=model_name, from_checkpoint=checkpoints[0], load_pretrained_weights=True,\n",
    "                       model_class=AutoModelForMaskedLM)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19f21eb8-cbf4-41c6-83a9-4340b82da024",
   "metadata": {},
   "source": [
    "# Loading data\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0dfaeaa-a693-400a-94bf-c013fe922d80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_opinion_lexicon():\n",
    "    with open(\"data/opinion-lexicon-English/negative-words.txt\", encoding=\"latin1\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [x.strip() for x in lines if not x.startswith(\";\")]\n",
    "    negative = [x for x in lines if len(x) > 0]\n",
    "    with open(\"data/opinion-lexicon-English/positive-words.txt\", encoding=\"latin1\") as f:\n",
    "        lines = f.readlines()\n",
    "    lines = [x.strip() for x in lines if not x.startswith(\";\")]\n",
    "    positive = [x for x in lines if len(x) > 0]\n",
    "    return positive, negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d5c4cd7-7ad8-4fd5-8e96-94547f25a165",
   "metadata": {},
   "outputs": [],
   "source": [
    "positive, negative = load_opinion_lexicon()\n",
    "# add the same tokens with an added whitespace in front for some tokenizers\n",
    "positive += [\" \" + x for x in positive]\n",
    "negative += [\" \" + x for x in negative]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603f642e-d507-4602-88a1-6da2c20ae493",
   "metadata": {},
   "outputs": [],
   "source": [
    "if intersect_vocabs == True:\n",
    "    if mask_models is None:\n",
    "        raise ValueError(\"Need a list of model names to load tokenizers for!\")\n",
    "    for model_name in mask_models:\n",
    "        t = AutoTokenizer.from_pretrained(model_name)\n",
    "        positive = [x for x in positive if len(t(x, add_special_tokens=False)[\"input_ids\"]) == 1]\n",
    "        negative = [x for x in negative if len(t(x, add_special_tokens=False)[\"input_ids\"]) == 1]\n",
    "\n",
    "    print(\"After intersecting vocabs of all models, we have\", len(positive),\" positive and\", len(negative), \"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff091d0-7875-4da6-a20e-29f9a302126d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_enc = {p:t for p,t in zip(positive, tokenizer(positive, add_special_tokens=False)[\"input_ids\"]) if len(t) == 1}\n",
    "neg_enc = {p:t for p,t in zip(negative, tokenizer(negative, add_special_tokens=False)[\"input_ids\"]) if len(t) == 1}\n",
    "\n",
    "pos_ids = sum(pos_enc.values(), [])\n",
    "neg_ids = sum(neg_enc.values(), [])\n",
    "\n",
    "all_ids = pos_ids + neg_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d290b56-e7e7-4ce7-95e7-254cf8def37c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Positive words:\", len(pos_ids))\n",
    "print(\"Negative words:\", len(neg_ids))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c35f1cf7-eb28-4aeb-ac29-d5582a0e757a",
   "metadata": {},
   "source": [
    "### Loading prompts\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4946ff5-16db-4813-bda2-e07d597b1cda",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prompt_files = [x for x in os.listdir(prompt_dir) if x.endswith(\".jsonl\")]\n",
    "dataset = datasets.DatasetDict()\n",
    "pos_label_word = next(iter(pos_enc.keys()))\n",
    "neg_label_word = next(iter(neg_enc.keys()))\n",
    "\n",
    "for pf in prompt_files:\n",
    "    d = pd.read_json(prompt_dir + pf, orient=\"records\", lines=True)\n",
    "    # 1: norm has positive moral judgment, 0 negative\n",
    "    d[\"original_label\"] = (d[\"action-moral-judgment\"] > 0).astype(\"int32\")\n",
    "    # [MASK] token needs to be replaced by actual mask token of the model\n",
    "    d[\"prompt\"] = d[\"prompt\"].apply(lambda x: x.replace(\"[MASK]\",tokenizer.mask_token))\n",
    "    # we create artificial text targets with a random positive or negative word.\n",
    "    # this way, we can infer whether an input should have been a positive or a negative norm during metric computation\n",
    "    d[\"label\"] = d.apply(lambda x: x[\"prompt\"].replace(tokenizer.mask_token, pos_label_word if x[\"original_label\"] == 1 else neg_label_word), axis=1)\n",
    "\n",
    "    dataset[os.path.splitext(pf)[0]] = datasets.Dataset.from_pandas(d)\n",
    "\n",
    "print(f\"Loaded {len(dataset)} prompt tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73e20fba-e1b4-40df-a24d-fcd9162cd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(samples):\n",
    "    return tokenizer(samples[\"prompt\"], text_target=samples[\"label\"], padding=False)\n",
    "\n",
    "tokenized_data = dataset.map(tokenize, batched=True, batch_size=1000)\n",
    "tokenized_data = tokenized_data.remove_columns([\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "376a06da-d953-4077-a91c-4e95516f1a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_metric\n",
    "import torch\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    probs = torch.tensor(eval_pred.predictions)\n",
    "    input_ids = torch.tensor(eval_pred.inputs)\n",
    "    \n",
    "    \n",
    "    y_pred = probs[input_ids == tokenizer.mask_token_id]\n",
    "        \n",
    "    # find out which label the samples had\n",
    "    labels = torch.tensor(eval_pred.label_ids)\n",
    "    labels = labels[input_ids == tokenizer.mask_token_id]\n",
    "    # if it is not a positive word id, then it is negative\n",
    "    # here we assume, that the input was genereated correctly\n",
    "    y_true = torch.isin(labels, torch.tensor(pos_ids))\n",
    "    acc = (y_true == y_pred).type(torch.float32).mean()\n",
    "\n",
    "    return {\"accuracy\":acc, \"y_pred\":y_pred.numpy()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c3996a5-34df-4982-9fd0-fe3a12430338",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we reduce the number of returned logits by 30kx fold to safe vram!\n",
    "pos_tensor = torch.tensor(pos_ids, device=model.device)\n",
    "neg_tensor = torch.tensor(neg_ids, device=model.device)\n",
    "\n",
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    probs = torch.softmax(logits, -1)\n",
    "    # lets pre-compute the sums of positive and negative probabilities\n",
    "    # this way, we only need to store [batch_size x seq_len x 2] bool values\n",
    "    # before this, we needed [batch_size x seq_len x vocab_size]\n",
    "    pos_probs = probs[:,:,pos_tensor].sum(axis=-1)\n",
    "    neg_probs = probs[:,:,neg_tensor].sum(axis=-1)\n",
    "    y_pred = pos_probs > neg_probs\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702f1a5f-399d-4025-ac62-8a8b14a956c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForTokenClassification\n",
    "\n",
    "dc = DataCollatorForTokenClassification(tokenizer, padding=True, pad_to_multiple_of=8, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1790105e-0ccc-491e-876b-7d1ac3eed474",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    data_collator=dc,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3876f6-51f6-4219-8df4-adcda5b5a5c5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "results = {}\n",
    "for split, data in tokenized_data.items():\n",
    "    r = trainer.evaluate(data, metric_key_prefix=f\"{split}\")\n",
    "    results[split] = r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b531f60-5944-487e-a9e8-56922852c9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "from collections import OrderedDict\n",
    "\n",
    "preds = OrderedDict({k:pd.DataFrame(v[f\"{k}_y_pred\"]) for k, v in results.items()})\n",
    "\n",
    "all_preds = reduce(lambda l,r: pd.concat([l,r], axis=1), preds.values())\n",
    "all_preds.columns = preds.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc9e92e1-c91e-4e11-81dc-d0c7b43ae28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(logdir + \"prompt_results.jsonl\", \"w\") as f:\n",
    "    f.write(all_preds.to_json(orient=\"records\", lines=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8f4f74-d6c4-4ef2-a19d-ea0acf82f768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.exit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733b75bf-3c44-4b43-b957-254a2f5a7077",
   "metadata": {},
   "source": [
    "# Prompting\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d82e686-97f0-4a4e-9fa7-e655154c23f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probs(batch):\n",
    "    inputs = tokenizer(batch[\"prompt\"], return_tensors=\"pt\", padding=True)\n",
    "    inputs = {k:v.to(model.device) for k,v in inputs.items()}\n",
    "    with torch.no_grad():\n",
    "        out = model(**inputs)\n",
    "    mask_logits = out.logits[torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)]\n",
    "    mask_probs = torch.softmax(mask_logits, 1, torch.float32)\n",
    "    pos_probs = mask_probs[:, pos_ids[1]].sum(axis=1).cpu().numpy()\n",
    "    neg_probs = mask_probs[:, neg_ids[1]].sum(axis=1).cpu().numpy()\n",
    "    pred = (pos_probs >= neg_probs).astype(\"int32\")\n",
    "    return {\"positive_sum\":pos_probs, \"negative_sum\":neg_probs, \"y_pred\": pred}\n",
    "\n",
    "def run_prompts(num_masks=1):\n",
    "    def g(batch):\n",
    "        # add number of masks to single mask prompts\n",
    "        prompts = [x.replace(tokenizer.mask_token, \" \".join([tokenizer.mask_token]*num_masks)) for x in batch[\"prompt\"]]\n",
    "                \n",
    "        inputs = tokenizer(prompts, return_tensors=\"pt\", padding=True)\n",
    "        inputs = {k:v.to(model.device) for k,v in inputs.items()}\n",
    "        with torch.no_grad():\n",
    "            out = model(**inputs)\n",
    "\n",
    "        # where are the masks?\n",
    "        mask_logits = out.logits[inputs[\"input_ids\"] == tokenizer.mask_token_id].reshape(len(prompts), -1, len(tokenizer))\n",
    "        mask_probs = torch.softmax(mask_logits, 2, torch.float32)\n",
    "        pos_probs = mask_probs[:, range(num_masks), pos_ids[num_masks]]\n",
    "        neg_probs = mask_probs[:, range(num_masks), neg_ids[num_masks]]\n",
    "\n",
    "        pos_probs_sum = pos_probs.sum([1,2])\n",
    "        neg_probs_sum = neg_probs.sum([1,2])\n",
    "        y_pred_sum = pos_probs_sum > neg_probs_sum\n",
    "\n",
    "        pos_probs_mean = pos_probs.mean([1,2])\n",
    "        neg_probs_mean = neg_probs.mean([1,2])\n",
    "        y_pred_mean = pos_probs_mean > neg_probs_mean \n",
    "        r = {\n",
    "            \"pos_probs_sum\": pos_probs_sum,\n",
    "            \"neg_probs_sum\": neg_probs_sum,\n",
    "            \"pos_probs_mean\": pos_probs_mean,\n",
    "            \"neg_probs_mean\": neg_probs_mean,\n",
    "            \"y_pred_sum\": y_pred_sum,\n",
    "            \"y_pred_mean\": y_pred_mean,\n",
    "            \"prompt\": prompts,\n",
    "        }\n",
    "        return {k:v.cpu().numpy() if isinstance(v, torch.Tensor) else v for k,v in r.items()}\n",
    "    return g\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8d543c6-06c6-4064-892d-0e3227acfdbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def run_ckpt(ckpt):\n",
    "    if ckpt is not None:\n",
    "        ckpt_log_dir = os.path.join(logdir, os.path.split(ckpt)[1])\n",
    "        # load checkpoint\n",
    "        print(\"loading checkpoint\")\n",
    "        x = fml.persistence.load_checkpoint(ckpt, model=model, prefer=\"hf\")\n",
    "\n",
    "    else:\n",
    "        ckpt_log_dir = logdir\n",
    "    print(ckpt_log_dir)\n",
    "    results = {}\n",
    "    for i in range(1, max_masks+1):\n",
    "        results[i] = dataset.map(run_prompts(num_masks=i), batched=True, batch_size=batch_size)\n",
    "\n",
    "    writer = SummaryWriter(log_dir=ckpt_log_dir)\n",
    "\n",
    "\n",
    "    for i in range(1, max_masks+1):\n",
    "        print(f\"Evaluating {i} masks prompts:\")\n",
    "        for split, data in results[i].items():\n",
    "            tag = f\"{i}_masks/{split}/\"\n",
    "\n",
    "            print(\"Run:\", tag)\n",
    "            data = data.to_pandas()\n",
    "            y = data[\"label\"]\n",
    "            y_pred = data[\"y_pred_sum\"]\n",
    "\n",
    "            f = plt.figure(figsize=(5,5))\n",
    "            ax = plt.gca()\n",
    "            ConfusionMatrixDisplay.from_predictions(y, y_pred, normalize=\"true\", display_labels=[\"bad\", \"good\"], ax=ax)\n",
    "            plt.title(split)\n",
    "            ax.xaxis.tick_top()\n",
    "            ax.xaxis.set_label_position('top')\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            writer.add_figure(tag+\"confusion\", f)\n",
    "\n",
    "            # classification metrics\n",
    "            report = classification_report(y, y_pred, output_dict=True)\n",
    "            for k,v in report.items():\n",
    "                if isinstance(v, dict):\n",
    "                    for metric, value in v.items():\n",
    "                        writer.add_scalar(f\"{tag}{k}/{metric}\", value)\n",
    "                else:\n",
    "                    writer.add_scalar(f\"{tag}{k}\", v)\n",
    "\n",
    "            print(classification_report(y, y_pred))\n",
    "            print(\"-\" * 60)\n",
    "    writer.flush()\n",
    "    writer.close()\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa60e19-4d2a-4c7c-935a-6a2ca25a7dff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for ckpt in checkpoints:\n",
    "    r = run_ckpt(ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a711c6c6-0f7c-432c-a9b5-70b6486db83f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95283d04-601c-491b-b0cb-bf5a3a4906ac",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
