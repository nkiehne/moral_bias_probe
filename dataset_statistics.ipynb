{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c81f496-453b-43f8-ab23-72977989c8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "import time\n",
    "import datasets\n",
    "import json\n",
    "\n",
    "\n",
    "pd.set_option('display.max_colwidth', 400)\n",
    "tasks = [\"moral-stories\",\"contrastive-moral-stories\",\"swag\",\"tweet-eval\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99d91d2e-44bd-41d2-996c-540f0151acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these words are considered as answers to the prompt and will be searched for in the output of the model\n",
    "positive_label_word = \"yes\"\n",
    "negative_label_word = \"no\"\n",
    "seed = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "0183dc35-9e5b-4109-8742-ba1690fb88c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def load_task(dataset):\n",
    "\n",
    "\n",
    "    def load_action_norm_split(path):\n",
    "        # loads the moral stories like data under path\n",
    "        train, dev, test = [pd.read_json(f\"{path}{x}.jsonl\", lines=True) for x in [\"train\", \"dev\", \"test\"]]\n",
    "\n",
    "        # construct dataframes that can actually be used\n",
    "        assign_action = lambda x: x[\"moral_action\"] if x[\"label\"] == 1 else x[\"immoral_action\"]\n",
    "        train[\"action\"] = train.apply(assign_action, axis=1)\n",
    "        dev[\"action\"] = dev.apply(assign_action, axis=1)\n",
    "        test[\"action\"] = test.apply(assign_action, axis=1)\n",
    "\n",
    "        subset = [\"norm\", \"action\", \"label\"]\n",
    "        train = train[subset]\n",
    "        dev = dev[subset]\n",
    "        test = test[subset]\n",
    "        return train, dev, test\n",
    "\n",
    "    if dataset==\"moral-stories\" or dataset == \"contrastive-moral-stories\":\n",
    "        if dataset == \"moral-stories\":\n",
    "            dataset_folder = \"data/contrastive_moral_stories/original_ms/action+norm/norm_distance/\"\n",
    "        else:\n",
    "            dataset_folder = \"data/contrastive_moral_stories/anti_ms_llama/action+norm/norm_distance/\"\n",
    "\n",
    "        train, dev, test = load_action_norm_split(dataset_folder)\n",
    "\n",
    "        # chat like prompt\n",
    "        t1 = \"Consider the norm \\\"{norm}\\\" and the action \\\"{action}\\\". Is the described action moral with respect to the norm? Answer with \\\"yes\\\" if it is moral or \\\"no\\\" if it is immoral.\\nAnswer:\"\n",
    "\n",
    "        print(t1.format(norm=\"N\", action=\"A\"))\n",
    "\n",
    "        def promptify(data, template):\n",
    "            data[\"prompt\"] =  [template.format(norm=n, action=a) for n,a in zip(data[\"norm\"], data[\"action\"])]\n",
    "            data[\"original_label\"] = data[\"label\"]\n",
    "            return data\n",
    "\n",
    "        train, dev, test = [promptify(x, t1) for x in [train, dev, test]]\n",
    "\n",
    "        data = datasets.DatasetDict()\n",
    "        data[\"train\"] = datasets.Dataset.from_pandas(train)\n",
    "        data[\"dev\"] = datasets.Dataset.from_pandas(dev)\n",
    "        data[\"test\"] = datasets.Dataset.from_pandas(test)\n",
    "\n",
    "    elif dataset==\"tweet-eval\":\n",
    "        te = load_dataset(\"tweet_eval\", \"hate\")\n",
    "        te[\"dev\"] = te.pop(\"validation\")\n",
    "\n",
    "        template = \"Here is a tweet: \\\"{tweet}\\\". If the tweet contains hate-speech, answer with \\\"yes\\\", or \\\"no\\\" if it doesn't.\\nAnswer:\"\n",
    "        print(template.format(tweet=\"T\"))\n",
    "\n",
    "        def promptify(data):\n",
    "            data[\"prompt\"] = [template.format(tweet=t) for t in data[\"text\"]]\n",
    "            data[\"original_label\"] = data[\"label\"]\n",
    "\n",
    "            return data\n",
    "\n",
    "        data = te.map(promptify, batched=True, batch_size=1000)\n",
    "\n",
    "    elif dataset==\"swag\":\n",
    "        from sklearn.model_selection import train_test_split\n",
    "\n",
    "        swag = load_dataset(\"swag\", \"regular\")\n",
    "\n",
    "        template = \"Does the ending fit the sentence?\\n{ctx}\\n{ending}\\n\\nAnswer:\"\n",
    "        print(template.format(ctx=\"CTX\", ending=\"END\"))\n",
    "\n",
    "        def prepare_data(data):\n",
    "            data[\"original_label\"] = data[\"label\"]\n",
    "            # if the correct answer is in the first two options, we use the sample as a positive one\n",
    "            # if not, then we use the very first option of the sample (always an incorrect option!) as a false sample\n",
    "\n",
    "            promptify = lambda row: template.format(ctx=row.startphrase, \n",
    "                                                    ending=row[f\"ending{row.original_label}\"] if row.original_label in {0,1} else row[\"ending0\"])\n",
    "            data[\"prompt\"] = data.apply(promptify, axis=1)\n",
    "            data[\"original_label\"] = data[\"original_label\"].apply(lambda x: int(x in {0,1}))\n",
    "            return data\n",
    "\n",
    "        train = swag[\"train\"].to_pandas()\n",
    "        test = swag[\"validation\"].to_pandas()\n",
    "        dev, test = train_test_split(test, test_size=0.5, shuffle=False, random_state=seed)\n",
    "\n",
    "        train, dev, test = [prepare_data(x) for x in [train, dev, test]]\n",
    "\n",
    "        data = datasets.DatasetDict()\n",
    "        data[\"train\"] = datasets.Dataset.from_pandas(train)\n",
    "        data[\"dev\"] = datasets.Dataset.from_pandas(dev)\n",
    "        data[\"test\"] = datasets.Dataset.from_pandas(test)\n",
    "\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown task '{dataset}'\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "da8a66c6-8915-4c4f-903f-70de547f2f6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_cde20\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_cde20_level0_col0\" class=\"col_heading level0 col0\" >#samples</th>\n",
       "      <th id=\"T_cde20_level0_col1\" class=\"col_heading level0 col1\" >#classes</th>\n",
       "      <th id=\"T_cde20_level0_col2\" class=\"col_heading level0 col2\" >class distr.#</th>\n",
       "      <th id=\"T_cde20_level0_col3\" class=\"col_heading level0 col3\" >Train/Dev/Test Split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_cde20_level0_row0\" class=\"row_heading level0 row0\" >moral-stories</th>\n",
       "      <td id=\"T_cde20_row0_col0\" class=\"data row0 col0\" >24000</td>\n",
       "      <td id=\"T_cde20_row0_col1\" class=\"data row0 col1\" >2</td>\n",
       "      <td id=\"T_cde20_row0_col2\" class=\"data row0 col2\" >0.50/0.50</td>\n",
       "      <td id=\"T_cde20_row0_col3\" class=\"data row0 col3\" >20000/2000/2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cde20_level0_row1\" class=\"row_heading level0 row1\" >contrastive-moral-stories</th>\n",
       "      <td id=\"T_cde20_row1_col0\" class=\"data row1 col0\" >24000</td>\n",
       "      <td id=\"T_cde20_row1_col1\" class=\"data row1 col1\" >2</td>\n",
       "      <td id=\"T_cde20_row1_col2\" class=\"data row1 col2\" >0.50/0.50</td>\n",
       "      <td id=\"T_cde20_row1_col3\" class=\"data row1 col3\" >20000/2000/2000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cde20_level0_row2\" class=\"row_heading level0 row2\" >swag</th>\n",
       "      <td id=\"T_cde20_row2_col0\" class=\"data row2 col0\" >93552</td>\n",
       "      <td id=\"T_cde20_row2_col1\" class=\"data row2 col1\" >2</td>\n",
       "      <td id=\"T_cde20_row2_col2\" class=\"data row2 col2\" >0.50/0.50</td>\n",
       "      <td id=\"T_cde20_row2_col3\" class=\"data row2 col3\" >73546/10003/10003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_cde20_level0_row3\" class=\"row_heading level0 row3\" >tweet-eval</th>\n",
       "      <td id=\"T_cde20_row3_col0\" class=\"data row3 col0\" >12970</td>\n",
       "      <td id=\"T_cde20_row3_col1\" class=\"data row3 col1\" >2</td>\n",
       "      <td id=\"T_cde20_row3_col2\" class=\"data row3 col2\" >0.58/0.42</td>\n",
       "      <td id=\"T_cde20_row3_col3\" class=\"data row3 col3\" >9000/1000/2970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7f96b2254490>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = pd.DataFrame(index=tasks)\n",
    "\n",
    "d[\"#samples\"] = 0\n",
    "d[\"#classes\"] = 0\n",
    "\n",
    "for ds, dataset in zip(tasks, data):\n",
    "    full = pd.concat([dataset[x].to_pandas() for x in [\"train\",\"dev\",\"test\"]])\n",
    "    ltrain, ldev, ltest = [len(dataset[x]) for x in [\"train\",\"dev\",\"test\"]]\n",
    "    d.loc[ds, \"#samples\"] = ltrain + ldev + ltest\n",
    "    d.loc[ds, \"#classes\"] = 2\n",
    "    \n",
    "    counts = full[\"original_label\"].value_counts(normalize=True)\n",
    "    d.loc[ds, \"class distr.#\"] = f\"{counts[0]:.2f}/{counts[1]:.2f}\"\n",
    "    \n",
    "    d.loc[ds, \"Train/Dev/Test Split\"] = f\"{ltrain}/{ldev}/{ltest}\"\n",
    "\n",
    "d.style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e465a64f-fd45-4288-ac3a-4d7d75fc3bd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrll}\n",
      " & #samples & #classes & class distr.# & Train/Dev/Test Split \\\\\n",
      "moral-stories & 24000 & 2 & 0.50/0.50 & 20000/2000/2000 \\\\\n",
      "contrastive-moral-stories & 24000 & 2 & 0.50/0.50 & 20000/2000/2000 \\\\\n",
      "swag & 93552 & 2 & 0.50/0.50 & 73546/10003/10003 \\\\\n",
      "tweet-eval & 12970 & 2 & 0.58/0.42 & 9000/1000/2970 \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(d.style.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f45011-4c05-4d1a-9550-841b256b6b84",
   "metadata": {},
   "source": [
    "# Few shot samples\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98bc4261-bf53-49bd-9652-4f9594d9f9ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lll}\n",
      " & Original & Labelled \\\\\n",
      "0 & not caring when your parent is crying. & \"If I were not caring when my parent is crying, that would be [MASK].\" \\\\\n",
      "1 & treating family members with respect & \"If I were treating family members with respect, that would be[MASK].\" \\\\\n",
      "2 & making other people sick. & \"If I were making other people sick, that would be[MASK].\" \\\\\n",
      "3 & not tipping in certain situations. & \"If I were not tipping in certain situations, that would be [MASK].\" \\\\\n",
      "4 & returning things that are lost & \"If I were returning things that are lost, that would be [MASK].\" \\\\\n",
      "5 & doing things together with your kids & \"If I were doing things with my kids, that would be [MASK].\" \\\\\n",
      "6 & intentionally disrupting someone when they're working. & \"If I were intentionally disrupting someone when they're working, that would be [MASK].\" \\\\\n",
      "7 & discussing sensitive topics in an English class. & \"If I were discussing sensitive topics in an English class, that would be [MASK].\" \\\\\n",
      "8 & Reporting child abuse to the authorities. & \"If I were reporting child abuse to the authorities, that would be [MASK].\" \\\\\n",
      "9 & distracting others from learning in school. & \"If I were distracting others from learning in school, that would be [MASK].\" \\\\\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from llama_prompt_gen import templates\n",
    "d = pd.DataFrame(columns=[\"Original\",\"Labelled\"], index=range(10))\n",
    "\n",
    "v = templates[\"prompt_hypothetical_first\"]\n",
    "split = [x for x in v.split(\"\\n\") if len(x) > 0]\n",
    "d[\"Original\"] = split[1:-1:2]\n",
    "d[\"Labelled\"] = split[2::2]\n",
    "print(d.style.to_latex())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f368eadf-a631-4167-9e91-83a2c3b7db03",
   "metadata": {},
   "source": [
    "# Samples for crowdsourcing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "e141bded-0776-4b0f-b7dc-0da8537b19c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prompt_hypothetical_third.jsonl\n",
      "prompt_original.jsonl\n",
      "prompt_reverse_qa.jsonl\n",
      "prompt_comma.jsonl\n",
      "prompt_hypothetical_second.jsonl\n",
      "prompt_hypothetical_first.jsonl\n",
      "prompt_instructive.jsonl\n",
      "prompt_no_cite.jsonl\n",
      "prompt_reverse.jsonl\n",
      "Loaded 9 prompt tasks\n"
     ]
    }
   ],
   "source": [
    "prompt_dir = \"data/prompts/\"\n",
    "prompt_files = [x for x in os.listdir(prompt_dir) if x.startswith(\"prompt\") and x.endswith(\".jsonl\")]\n",
    "\n",
    "dfs = []\n",
    "\n",
    "for pf in prompt_files:\n",
    "    d = pd.read_json(prompt_dir + pf, orient=\"records\", lines=True)\n",
    "    if len(d) < 1000: continue\n",
    "    # 1: norm has positive moral judgment, 0 negative\n",
    "    d[\"original_label\"] = (d[\"action-moral-judgment\"] > 0).astype(\"int32\")\n",
    "\n",
    "    print(pf)\n",
    "    dfs.append(d)\n",
    "\n",
    "print(f\"Loaded {len(dfs)} prompt tasks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "feac1aec-b539-4aa4-b2d5-84d7329c425e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find random samples\n",
    "random_sample = d.sample(100, random_state=8197).index\n",
    "\n",
    "dfs = [d.loc[random_sample,[\"norm\",\"prompt\"]] for d in dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "a715d8fe-a657-4017-b2c1-a4d9ea6e0986",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want no batched samples...\n",
    "for k, v in zip(prompt_files, dfs):\n",
    "    v[\"label\"] = \"\"\n",
    "    v.to_csv(\"data/prompts/human-eval/\"+k.removesuffix(\".jsonl\")+\".csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a8eb54f-6edb-40ae-b162-f94e2660c56f",
   "metadata": {},
   "source": [
    "# Model sizes\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "316e9dc5-2913-4c7f-9544-6c9fbc90bae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = ['distilbert-base-uncased',\n",
    " 'bert-base-uncased',\n",
    " 'bert-large-uncased',\n",
    " 'bert-base-multilingual-uncased',\n",
    " 'distilroberta-base',\n",
    " 'roberta-base',\n",
    " 'roberta-large',\n",
    " 'xlm-roberta-base',\n",
    " 'xlm-roberta-large',\n",
    " 'albert-xxlarge-v2',\n",
    " 'gpt2',\n",
    " 'gpt2-large',\n",
    " 'gpt2-xl',\n",
    " 'llama7B',\n",
    " 'EleutherAI/gpt-neo-2.7B']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "73a52ef0-664d-4883-abf0-1b8cb0eee68f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilbert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_projector.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-large-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-large-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert-base-multilingual-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-uncased were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "distilroberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilroberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-large were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-base\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xlm-roberta-large\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at xlm-roberta-large were not used when initializing XLMRobertaModel: ['lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.bias', 'lm_head.layer_norm.bias']\n",
      "- This IS expected if you are initializing XLMRobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing XLMRobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "albert-xxlarge-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at albert-xxlarge-v2 were not used when initializing AlbertModel: ['predictions.LayerNorm.bias', 'predictions.dense.weight', 'predictions.dense.bias', 'predictions.decoder.weight', 'predictions.bias', 'predictions.LayerNorm.weight', 'predictions.decoder.bias']\n",
      "- This IS expected if you are initializing AlbertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing AlbertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpt2\n",
      "gpt2-large\n",
      "gpt2-xl\n",
      "llama7B\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c33ebd6101a44546b03592baa8798eb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at llama7B were not used when initializing LlamaModel: ['lm_head.weight']\n",
      "- This IS expected if you are initializing LlamaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing LlamaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EleutherAI/gpt-neo-2.7B\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoModel were not initialized from the model checkpoint at EleutherAI/gpt-neo-2.7B and are newly initialized: ['transformer.h.11.attn.attention.bias', 'transformer.h.23.attn.attention.bias', 'transformer.h.17.attn.attention.bias', 'transformer.h.3.attn.attention.bias', 'transformer.h.15.attn.attention.bias', 'transformer.h.21.attn.attention.bias', 'transformer.h.27.attn.attention.bias', 'transformer.h.31.attn.attention.bias', 'transformer.h.29.attn.attention.bias', 'transformer.h.13.attn.attention.bias', 'transformer.h.1.attn.attention.bias', 'transformer.h.25.attn.attention.bias', 'transformer.h.7.attn.attention.bias', 'transformer.h.19.attn.attention.bias', 'transformer.h.5.attn.attention.bias', 'transformer.h.9.attn.attention.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModel\n",
    "r = {}\n",
    "for m in models:\n",
    "    print(m)\n",
    "    model = AutoModel.from_pretrained(m)\n",
    "    r[m] = model.num_parameters()\n",
    "    del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6818e939-4505-4658-938a-fd8ba44f8b19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('distilbert-base-uncased', 66362880),\n",
       " ('distilroberta-base', 82118400),\n",
       " ('bert-base-uncased', 109482240),\n",
       " ('gpt2', 124439808),\n",
       " ('roberta-base', 124645632),\n",
       " ('bert-base-multilingual-uncased', 167356416),\n",
       " ('albert-xxlarge-v2', 222595584),\n",
       " ('xlm-roberta-base', 278043648),\n",
       " ('bert-large-uncased', 335141888),\n",
       " ('roberta-large', 355359744),\n",
       " ('xlm-roberta-large', 559890432),\n",
       " ('gpt2-large', 774030080),\n",
       " ('gpt2-xl', 1557611200),\n",
       " ('EleutherAI/gpt-neo-2.7B', 2651307520),\n",
       " ('llama7B', 6607343616)]"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(r.items(), key=lambda x: r[x[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
